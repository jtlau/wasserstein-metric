{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "from geomloss import SamplesLoss\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "import h5py  \n",
    "import pandas as pd\n",
    "from scipy.io import savemat\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf5_snapshot(fname, var=\"snapshot_matrix_000000\"):\n",
    "    hf = h5py.File(fname, 'r')\n",
    "    snap = np.array(hf[var][:])\n",
    "    return snap\n",
    "\n",
    "def find_states(full_sol, N, nvar=4):\n",
    "    # decomposes a state solution into its variable solutions\n",
    "    var1 = np.zeros(N,)\n",
    "    var2 = np.zeros(N,)\n",
    "    var3 = np.zeros(N,)\n",
    "    var4 = np.zeros(N,)\n",
    "    \n",
    "    for i in range(N):\n",
    "        var1[i] = full_sol[(i)*nvar + 0];\n",
    "        var2[i] = full_sol[(i)*nvar + 1];\n",
    "        var3[i] = full_sol[(i)*nvar + 2];\n",
    "        var4[i] = full_sol[(i)*nvar + 3];\n",
    "    \n",
    "    return var1, var2, var3, var4\n",
    "    \n",
    "def create_weights(state, psize, mn):\n",
    "    state2 = (state - mn)*psize\n",
    "    state2 = state2 / 10\n",
    "    state2 = state2.round()\n",
    "    return state2\n",
    "\n",
    "def find_pebble_size(state):\n",
    "    avg = sum(state) / len(state)\n",
    "    mn = min(state)\n",
    "    return psize\n",
    "    \n",
    "    \n",
    "# remove rows with zero difference\n",
    "def remove_zeros(x1, x2, mesh):\n",
    "    x3 = x1 - x2\n",
    "    x1 = x1[np.where(x3 != 0)]\n",
    "    x2 = x2[np.where(x3 != 0)]\n",
    "    mesh = mesh[np.where(x3 != 0)]\n",
    "    \n",
    "    return x1, x2, mesh\n",
    "\n",
    "def create_pebbles(x1, mesh, npebbles):\n",
    "    # each pebble is denoted by its location in the mesh  \n",
    "    mesh1 = np.empty([int(sum(x1)), 2])\n",
    "    k1 = 0\n",
    "    for i in range(len(mesh)):\n",
    "        n1 = int(x1[i])\n",
    "        mesh1[k1:(k1+n1)] = mesh[i]\n",
    "        k1 += n1\n",
    "    \n",
    "    mesh1 = np.array(choices(mesh1, k=npebbles))\n",
    "    \n",
    "    return mesh1 \n",
    "\n",
    "def display_samples(ax, x, color):\n",
    "    x_ = x.detach().cpu().numpy()\n",
    "    ax.scatter(x_[:, 0], x_[:, 1], 25 * 500 / len(x_), color, edgecolors=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from tutorials at:\n",
    "# https://www.kernel-operations.io/geomloss/_auto_examples/\n",
    "#         optimal_transport/plot_optimal_transport_2D.html\n",
    "#         #sphx-glr-auto-examples-optimal-transport-plot-optimal-transport-2d-py\n",
    "\n",
    "def gradient_descent(loss, Nsteps = 11, lr=1, disp=1):\n",
    "    \"\"\"Flows along the gradient of the loss function.\n",
    "\n",
    "    Parameters:\n",
    "        loss ((x_i,y_j) -> torch float number):\n",
    "            Real-valued loss function.\n",
    "        lr (float, default = 1):\n",
    "            Learning rate, i.e. time step.\n",
    "    \"\"\"\n",
    "    \n",
    "    display_its = [0, 1, 4, Nsteps-1]\n",
    "    \n",
    "    # Use colors to identify the particles\n",
    "    colors = (70 * X_i[:, 0]).cos() * (70 * X_i[:, 1]).cos()\n",
    "    colors = colors.detach().cpu().numpy()\n",
    "    \n",
    "    # Make sure that we won't modify the reference samples\n",
    "    x_i, y_j = X_i.clone(), Y_j.clone()\n",
    "    \n",
    "    # We're going to perform gradient descent on Loss(α, β)\n",
    "    # wrt. the positions x_i of the diracs masses that make up α:\n",
    "    x_i.requires_grad = True\n",
    "    \n",
    "    t_0 = time.time()\n",
    "    if (disp==1):\n",
    "        plt.figure(figsize=(6, 6))\n",
    "    k = 1\n",
    "    for i in range(Nsteps):  # Euler scheme ===============\n",
    "        # Compute cost and gradient\n",
    "        L_αβ = loss(x_i, y_j)\n",
    "        [g] = torch.autograd.grad(L_αβ, [x_i])\n",
    "        \n",
    "        if (i in display_its) and (disp == 1):  # display\n",
    "            #ax = plt.subplot(2, 2, k)\n",
    "            ax = plt.subplot(1,1,1)\n",
    "            k = k + 1\n",
    "            plt.set_cmap(\"hsv\")\n",
    "            plt.scatter(\n",
    "                [10], [10]\n",
    "            )  # shameless hack to prevent a slight change of axis...\n",
    "            display_samples(ax, y_j, [(0.55, 0.55, 0.95)])\n",
    "            display_samples(ax, x_i, colors)\n",
    "            ax.set_title(\"it = {}\".format(i))\n",
    "            plt.axis([0, 1.69, -0.35, 0.35])\n",
    "            plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "            plt.xticks([], [])\n",
    "            plt.yticks([], [])\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'wass_mach_AE_{i}.png',transparent = False, facecolor = 'white')\n",
    "            plt.clf()\n",
    "\n",
    "        # in-place modification of the tensor's values\n",
    "        x_i.data -= lr * len(x_i) * g\n",
    "        \n",
    "    return x_i.detach().cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset \n",
    "snap1 = load_hdf5_snapshot(\"data/mach3p5/su2_snapshot.000000\")\n",
    "snap2 = load_hdf5_snapshot(\"data/mach2p5/su2_snapshot.000000\")\n",
    "# mesh points\n",
    "xy = np.loadtxt(open(\"data/xy.csv\", \"rb\"), delimiter=\",\")\n",
    "xy = xy[:,0:2]\n",
    "\n",
    "# decompose full state vector into variable vectors\n",
    "#xi, var12, var13, var14 = find_states(snap1, N, 4)\n",
    "#yj, var22, var23, var24 = find_states(snap2, N, 4)\n",
    "xi = snap1\n",
    "yj = snap2\n",
    "\n",
    "# parameters\n",
    "tol = 1e-1  # tolerance for comparison of the two datasets (problem dependent)\n",
    "M = 5000     # number of (randomly sampled) pebbles to use in algorithm (5000 max if using CPU)\n",
    "\n",
    "# pre-process dataset\n",
    "Xi = create_weights(xi, 1/tol, min(yj))\n",
    "Yj = create_weights(yj, 1/tol, min(yj))\n",
    "\n",
    "Xi, Yj, xy = remove_zeros(Xi, Yj, xy)\n",
    "\n",
    "M2 = min(round(len(Xi)),M)\n",
    "print(\"M: {}\".format(M2))\n",
    "\n",
    "X_i = create_pebbles(Xi, xy, M2)\n",
    "Y_j = create_pebbles(Yj, xy, M2)\n",
    "\n",
    "X_i = torch.from_numpy(X_i).type(torch.FloatTensor)\n",
    "Y_j = torch.from_numpy(Y_j).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfinal = gradient_descent(SamplesLoss(\"sinkhorn\", p=2, blur=0.01),15)\n",
    "\n",
    "# find wasserstein cost\n",
    "xinit = X_i.detach().cpu().numpy()\n",
    "dw = np.sqrt( sum((xinit[:,0] - xfinal[:,0])**2 + (xinit[:,1] - xfinal[:,1])**2)) \n",
    "dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein cost for multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over simulation data in parameter space\n",
    "files = [\"mach2\", \"mach3\", \"mach3p5\", \"mach4\"]\n",
    "xy_orig = np.loadtxt(open(\"data/xy.csv\", \"rb\"), delimiter=\",\")\n",
    "xy_orig = xy_orig[:,0:2]\n",
    "\n",
    "snap2 = load_hdf5_snapshot(\"data/mach2p5/su2_snapshot.000000\")\n",
    "yj, var22, var23, var24 = find_states(snap2, 20200, 4)\n",
    "\n",
    "# parameters\n",
    "tol = 1e-3  # tolerance for comparison of the two datasets\n",
    "M = 2000    # number of (randomly sampled) pebbles to use in algorithm\n",
    "Niter = 5   # number of gradient descent iterations\n",
    "\n",
    "d = np.empty([len(files), 1])\n",
    "\n",
    "for i in range(len(files)):\n",
    "    # dataset \n",
    "    snap1 = load_hdf5_snapshot(\"data/\" + files[i] + \"/su2_snapshot.000000\")\n",
    "\n",
    "    # decompose full state vector into variable vectors\n",
    "    xi, var12, var13, var14 = find_states(snap1, 20200, 4)\n",
    "    \n",
    "    # pre-process dataset\n",
    "    Xi = create_weights(xi, 1/tol, min(yj))\n",
    "    Yj = create_weights(yj, 1/tol, min(yj))\n",
    "    \n",
    "    Xi, Yj, xy = remove_zeros(Xi, Yj, xy_orig)\n",
    "    \n",
    "    M2 = min(round(len(Xi)),M)\n",
    "    \n",
    "    X_i = create_pebbles(Xi, xy, M2)\n",
    "    X_i = torch.from_numpy(X_i).type(torch.FloatTensor)\n",
    "    \n",
    "    if (i==0):\n",
    "        Y_j = create_pebbles(Yj, xy, M2)\n",
    "        Y_j = torch.from_numpy(Y_j).type(torch.FloatTensor)\n",
    "    \n",
    "    print(\"Parameter: {}, M: {}\".format(files[i], M2))\n",
    "    st = time.time()\n",
    "    \n",
    "    xfinal = gradient_descent(SamplesLoss(\"sinkhorn\", p=2, blur=0.01), Niter, disp=0)\n",
    "    \n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "    print('Execution time:', elapsed_time, 'seconds')\n",
    "    \n",
    "    # find wasserstein cost\n",
    "    xinit = X_i.detach().cpu().numpy()\n",
    "    d[i] = np.linalg.norm(xinit - xfinal)\n",
    "    #d[i] = sum(np.sqrt((xinit[:,0] - xfinal[:,0])**2 + (xinit[:,1] - xfinal[:,1])**2))/ M2\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "labels = [2, 3, 3.5, 4]\n",
    "plt.plot(labels, d)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "#ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
